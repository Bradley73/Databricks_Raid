{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f66965-d175-4dd8-8128-8636c2de899e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "from raid.schemas.champindex.schema import DATA_SCHEMA, BRONZE_COLS, SCHEMA_VERSION\n",
    "\n",
    "# NEW: shared ops logging\n",
    "from ops.ingest_logging import write_log_best_effort\n",
    "from ops.ingest_log_context import LogContext\n",
    "from ops.ingest_log_constants import IngestStatus\n",
    "from ops.ingest_log_builders import run_event, file_success_events\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TARGET_TABLE_FQN = \"raid.bronze_champindex\"\n",
    "PIPELINE_NAME = \"01_ingest_bronze\"\n",
    "LAYER = \"bronze\"\n",
    "\n",
    "RUN_ID = str(uuid.uuid4())\n",
    "PIPELINE_RUN_TS = datetime.utcnow()\n",
    "\n",
    "LANDING_BASE = \"/Workspace/raid/tables/landing\"\n",
    "SOURCE_GLOB = f\"{LANDING_BASE}/AccountName=*/champindex_*.csv\"\n",
    "\n",
    "SCHEMA_LOCATION = \"/Workspace/raid/schemas/autoloader/champindex_bronze\"\n",
    "CHECKPOINT_LOCATION = \"/Workspace/raid/checkpoints/autoloader/01_ingest_bronze\"\n",
    "\n",
    "CSV_OPTS = {\n",
    "    \"header\": \"true\",\n",
    "    \"sep\": \";\",\n",
    "    \"quote\": '\"',\n",
    "    \"escape\": '\"',\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "# Initialise log context\n",
    "LOG_CTX = LogContext(\n",
    "    target_table_fqn=TARGET_TABLE_FQN,\n",
    "    layer=LAYER,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    run_id=RUN_ID,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    schema_location=SCHEMA_LOCATION,\n",
    "    base_context={\n",
    "        \"trigger\": \"availableNow\",\n",
    "        \"source_glob\": SOURCE_GLOB,\n",
    "    },\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers (path parsing)\n",
    "# -----------------------\n",
    "source_file_col = F.input_file_name()\n",
    "file_name_col = F.element_at(F.split(source_file_col, \"/\"), -1)\n",
    "\n",
    "AccountName_col = F.regexp_extract(source_file_col, r\"/AccountName=([^/]+)/\", 1)\n",
    "\n",
    "ddmmyyyy_col = F.regexp_extract(\n",
    "    file_name_col,\n",
    "    r\"^champindex_[^_]+_(\\d{8})_\\d+\\.csv$\",\n",
    "    1\n",
    ")\n",
    "snapshot_date_col = F.to_date(ddmmyyyy_col, \"ddMMyyyy\")\n",
    "\n",
    "# -----------------------\n",
    "# Auto Loader read\n",
    "# -----------------------\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION)\n",
    "        .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "        .options(**CSV_OPTS)\n",
    "        .schema(DATA_SCHEMA)\n",
    "        .load(SOURCE_GLOB)\n",
    ")\n",
    "\n",
    "# Shape bronze once (this DF is what the foreachBatch receives)\n",
    "bronze_df = (\n",
    "    raw_stream\n",
    "        .withColumn(\"source_file\", source_file_col)\n",
    "        .withColumn(\"AccountName\", AccountName_col)\n",
    "        .withColumn(\"snapshot_ts\", F.current_timestamp())\n",
    "        .withColumn(\"snapshot_date\", snapshot_date_col)\n",
    "        .withColumn(\"schema_version\", F.lit(SCHEMA_VERSION))\n",
    "        .withColumn(\"run_id\", F.lit(RUN_ID))   # IMPORTANT: before select(*BRONZE_COLS)\n",
    "        .select(*BRONZE_COLS)\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# END OF INGESTION / TRANSFORMATION LOGIC\n",
    "# ---------------------------------------------------------------------\n",
    "# Below this point:\n",
    "#   - No schema or parsing changes should be made\n",
    "#   - DataFrames are considered FINAL for writing\n",
    "#   - Only execution concerns are handled:\n",
    "#       * writing to bronze\n",
    "#       * ingestion logging\n",
    "#       * error handling\n",
    "# =====================================================================\n",
    "\n",
    "def process_batch(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    started_ts = F.current_timestamp()\n",
    "    run_ts = F.lit(PIPELINE_RUN_TS)\n",
    "\n",
    "    # EMPTY -> RUN EMPTY then return\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.EMPTY,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=F.current_timestamp(),\n",
    "                message=\"No new data files in this batch\",\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # RUN STARTED\n",
    "    write_log_best_effort(\n",
    "        run_event(\n",
    "            spark, LOG_CTX,\n",
    "            pipeline_run_ts_col=run_ts,\n",
    "            batch_id=int(batch_id),\n",
    "            status=IngestStatus.STARTED,\n",
    "            started_ts_col=started_ts,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Write bronze (unchanged)\n",
    "        (\n",
    "            batch_df.write\n",
    "              .format(\"delta\")\n",
    "              .mode(\"append\")\n",
    "              .saveAsTable(TARGET_TABLE_FQN)\n",
    "        )\n",
    "\n",
    "        finished_ts = F.current_timestamp()\n",
    "\n",
    "        # FILE SUCCESS rows\n",
    "        write_log_best_effort(\n",
    "            file_success_events(\n",
    "                batch_df, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                rescued_col=\"_rescued_data\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # RUN SUCCESS\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.SUCCESS,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                message=\"Batch completed successfully\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        finished_ts = F.current_timestamp()\n",
    "\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.FAILED,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                message=str(e)[:4000],\n",
    "                error_class=e.__class__.__name__,\n",
    "            )\n",
    "        )\n",
    "        raise\n",
    "\n",
    "query = (\n",
    "    bronze_df.writeStream\n",
    "      .foreachBatch(process_batch)\n",
    "      .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    "      .trigger(availableNow=True)\n",
    "      .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
