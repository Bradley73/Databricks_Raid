{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f66965-d175-4dd8-8128-8636c2de899e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# stdlib\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "\n",
    "# raid (project-specific)\n",
    "from projects.raid.schema.champindex.schema import (\n",
    "    DATA_SCHEMA,\n",
    "    BRONZE_COLS,\n",
    "    SCHEMA_VERSION,\n",
    ")\n",
    "\n",
    "# shared logging / ops\n",
    "from shared.logging.ingest_logging import write_log_best_effort\n",
    "from shared.logging.ingest_log_constants import IngestStatus\n",
    "from shared.logging.ingest_log_context import LogContext\n",
    "from shared.logging.ingest_log_builders import (\n",
    "    run_event,\n",
    "    file_success_events,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TARGET_TABLE_FQN = \"raid.bronze_champindex\"\n",
    "PIPELINE_NAME = \"01_ingest_bronze\"\n",
    "LAYER = \"bronze\"\n",
    "\n",
    "RUN_ID = str(uuid.uuid4())\n",
    "PIPELINE_RUN_TS = datetime.now(timezone.utc)\n",
    "\n",
    "LANDING_BASE = \"/Volumes/workspace/raid/champindex\"\n",
    "SOURCE_GLOB = f\"{LANDING_BASE}/AccountName=*/champindex_*.csv\"\n",
    "\n",
    "SCHEMA_LOCATION = \"/Volumes/workspace/raid/_system/autoloader_schemas/champindex_bronze\"\n",
    "CHECKPOINT_LOCATION = \"/Volumes/workspace/raid/_system/checkpoints/01_ingest_bronze/champindex\"\n",
    "\n",
    "CSV_OPTS = {\n",
    "    \"header\": \"true\",\n",
    "    \"sep\": \";\",\n",
    "    \"quote\": '\"',\n",
    "    \"escape\": '\"',\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "# Initialise log context\n",
    "LOG_CTX = LogContext(\n",
    "    target_table_fqn=TARGET_TABLE_FQN,\n",
    "    layer=LAYER,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    run_id=RUN_ID,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    schema_location=SCHEMA_LOCATION,\n",
    "    base_context={\n",
    "        \"trigger\": \"availableNow\",\n",
    "        \"source_glob\": SOURCE_GLOB,\n",
    "    },\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers (path parsing)\n",
    "# -----------------------\n",
    "source_file_col = F.col(\"_metadata.file_path\")\n",
    "file_name_col = F.element_at(F.split(source_file_col, \"/\"), -1)\n",
    "\n",
    "AccountName_col = F.regexp_extract(source_file_col, r\"/AccountName=([^/]+)/\", 1)\n",
    "\n",
    "ddmmyyyy_col = F.regexp_extract(file_name_col, r\"^champindex_[^_]+_(\\d{8})_\\d+\\.csv$\", 1)\n",
    "snapshot_date_col = F.try_to_date(ddmmyyyy_col, \"ddMMyyyy\")\n",
    "\n",
    "# -----------------------\n",
    "# Auto Loader read\n",
    "# -----------------------\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION)\n",
    "        .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"ignoreMissingFiles\", \"true\")\n",
    "        .options(**CSV_OPTS)\n",
    "        .schema(DATA_SCHEMA)\n",
    "        .load(LANDING_BASE)\n",
    ")\n",
    "\n",
    "# Shape bronze once (this DF is what the foreachBatch receives)\n",
    "bronze_df = (\n",
    "    raw_stream\n",
    "        .withColumn(\"source_file\", source_file_col)\n",
    "        .withColumn(\"AccountName\", AccountName_col)\n",
    "        .withColumn(\"snapshot_ts\", F.current_timestamp())\n",
    "        .withColumn(\"snapshot_date\", snapshot_date_col)\n",
    "        .withColumn(\"schema_version\", F.lit(SCHEMA_VERSION))\n",
    "        .withColumn(\"run_id\", F.lit(RUN_ID))   # IMPORTANT: before select(*BRONZE_COLS)\n",
    "        .select(*BRONZE_COLS)\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# END OF INGESTION / TRANSFORMATION LOGIC\n",
    "# ---------------------------------------------------------------------\n",
    "# Below this point:\n",
    "#   - No schema or parsing changes should be made\n",
    "#   - DataFrames are considered FINAL for writing\n",
    "#   - Only execution concerns are handled:\n",
    "#       * writing to bronze\n",
    "#       * ingestion logging\n",
    "#       * error handling\n",
    "# =====================================================================\n",
    "\n",
    "def process_batch(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    started_ts = F.current_timestamp()\n",
    "    run_ts = F.lit(PIPELINE_RUN_TS)\n",
    "\n",
    "    # EMPTY -> RUN EMPTY then return\n",
    "    is_empty = batch_df.limit(1).count() == 0\n",
    "    if is_empty:\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.EMPTY,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=F.current_timestamp(),\n",
    "                message=\"No new data files in this batch\",\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # RUN STARTED\n",
    "    write_log_best_effort(\n",
    "        run_event(\n",
    "            spark, LOG_CTX,\n",
    "            pipeline_run_ts_col=run_ts,\n",
    "            batch_id=int(batch_id),\n",
    "            status=IngestStatus.STARTED,\n",
    "            started_ts_col=started_ts,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Write bronze (unchanged)\n",
    "        (\n",
    "            batch_df.write\n",
    "              .format(\"delta\")\n",
    "              .mode(\"append\")\n",
    "              .saveAsTable(TARGET_TABLE_FQN)\n",
    "        )\n",
    "\n",
    "        finished_ts = F.current_timestamp()\n",
    "\n",
    "        # FILE SUCCESS rows\n",
    "        write_log_best_effort(\n",
    "            file_success_events(\n",
    "                batch_df, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                rescued_col=\"_rescued_data\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # RUN SUCCESS\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.SUCCESS,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                message=\"Batch completed successfully\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        finished_ts = F.current_timestamp()\n",
    "\n",
    "        write_log_best_effort(\n",
    "            run_event(\n",
    "                spark, LOG_CTX,\n",
    "                pipeline_run_ts_col=run_ts,\n",
    "                batch_id=int(batch_id),\n",
    "                status=IngestStatus.FAILED,\n",
    "                started_ts_col=started_ts,\n",
    "                finished_ts_col=finished_ts,\n",
    "                message=str(e)[:4000],\n",
    "                error_class=e.__class__.__name__,\n",
    "            )\n",
    "        )\n",
    "        raise\n",
    "\n",
    "query = (\n",
    "    bronze_df.writeStream\n",
    "      .foreachBatch(process_batch)\n",
    "      .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    "      .trigger(availableNow=True)\n",
    "      .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac9ab55-a56d-4b0d-b8b4-90559d4319e2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":52},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766878921947}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM ops.ingest_log"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5652313907494998,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
