{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f66965-d175-4dd8-8128-8636c2de899e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# stdlib\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "\n",
    "# raid (project-specific)\n",
    "from projects.raid.schema.champindex.schema import (\n",
    "    DATA_SCHEMA,\n",
    "    BRONZE_COLS,\n",
    "    SCHEMA_VERSION,\n",
    ")\n",
    "\n",
    "# shared logging / ops\n",
    "from shared.logging.ingest_logging import write_log_best_effort\n",
    "from shared.logging.ingest_log_constants import IngestStatus\n",
    "from shared.logging.ingest_log_context import LogContext\n",
    "from shared.logging.ingest_log_builders import (\n",
    "    run_event,\n",
    "    file_success_events,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TARGET_TABLE_FQN = \"raid.bronze_champindex\"\n",
    "PIPELINE_NAME = \"01_ingest_bronze\"\n",
    "LAYER = \"bronze\"\n",
    "\n",
    "RUN_ID = str(uuid.uuid4())\n",
    "PIPELINE_RUN_TS_PY = datetime.now(timezone.utc) # static timestamp of run start\n",
    "PIPELINE_RUN_TS = F.lit(PIPELINE_RUN_TS_PY).cast(\"timestamp\") # convert to spark col format\n",
    "\n",
    "LANDING_BASE = \"/Volumes/workspace/raid/champindex\"\n",
    "\n",
    "SCHEMA_LOCATION = \"/Volumes/workspace/raid/_system/autoloader_schemas/champindex_bronze\"\n",
    "CHECKPOINT_LOCATION = \"/Volumes/workspace/raid/_system/checkpoints/01_ingest_bronze/champindex\"\n",
    "\n",
    "CSV_OPTS = {\n",
    "    \"header\": \"true\",\n",
    "    \"sep\": \";\",\n",
    "    \"quote\": '\"',\n",
    "    \"escape\": '\"',\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "# Initialise log context\n",
    "LOG_CTX = LogContext(\n",
    "    target_table_fqn=TARGET_TABLE_FQN,\n",
    "    layer=LAYER,\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    run_id=RUN_ID,\n",
    "    checkpoint_location=CHECKPOINT_LOCATION,\n",
    "    schema_location=SCHEMA_LOCATION,\n",
    "    base_context={\n",
    "        \"trigger\": \"availableNow\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers (path parsing)\n",
    "# -----------------------\n",
    "source_file_col = F.col(\"_metadata.file_path\")\n",
    "file_name_col = F.element_at(F.split(source_file_col, \"/\"), -1)\n",
    "\n",
    "AccountName_col = F.regexp_extract(source_file_col, r\"/AccountName=([^/]+)/\", 1)\n",
    "\n",
    "ddmmyyyy_col = F.regexp_extract(file_name_col, r\"^champindex_[^_]+_(\\d{8})_\\d+\\.csv$\", 1)\n",
    "snapshot_date_col = F.try_to_date(ddmmyyyy_col, \"ddMMyyyy\")\n",
    "\n",
    "# -----------------------\n",
    "# Auto Loader read\n",
    "# -----------------------\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION)\n",
    "        .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"ignoreMissingFiles\", \"true\")\n",
    "        .options(**CSV_OPTS)\n",
    "        .schema(DATA_SCHEMA)\n",
    "        .load(LANDING_BASE)\n",
    ")\n",
    "\n",
    "# Shape bronze once (this DF is what the foreachBatch receives)\n",
    "bronze_df = (\n",
    "    raw_stream\n",
    "        .withColumn(\"source_file\", source_file_col)\n",
    "        .withColumn(\"AccountName\", AccountName_col)\n",
    "        .withColumn(\"snapshot_ts\", F.current_timestamp())\n",
    "        .withColumn(\"snapshot_date\", snapshot_date_col)\n",
    "        .withColumn(\"schema_version\", F.lit(SCHEMA_VERSION))\n",
    "        .withColumn(\"run_id\", F.lit(RUN_ID))   # IMPORTANT: before select(*BRONZE_COLS)\n",
    "        .select(*BRONZE_COLS)\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# END OF INGESTION / TRANSFORMATION LOGIC\n",
    "# ---------------------------------------------------------------------\n",
    "# Below this point:\n",
    "#   - No schema or parsing changes should be made\n",
    "#   - DataFrames are considered FINAL for writing\n",
    "#   - Only execution concerns are handled:\n",
    "#       * writing to bronze\n",
    "#       * ingestion logging\n",
    "#       * error handling\n",
    "# =====================================================================\n",
    "\n",
    "# -----------------------\n",
    "# foreachBatch: write + FILE events only\n",
    "# -----------------------\n",
    "def process_batch(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    started_ts = datetime.now(timezone.utc)\n",
    "\n",
    "    # NOTE: empty micro-batches are ignored; RUN status is derived from recentProgress\n",
    "    if not batch_df.limit(1).collect():\n",
    "        return\n",
    "\n",
    "    # Write bronze\n",
    "    (\n",
    "        batch_df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(TARGET_TABLE_FQN)\n",
    "    )\n",
    "\n",
    "    finished_ts = datetime.now(timezone.utc)\n",
    "\n",
    "    # FILE SUCCESS rows (per file in this batch)\n",
    "    write_log_best_effort(\n",
    "        file_success_events(\n",
    "            batch_df, LOG_CTX,\n",
    "            pipeline_run_ts_col=PIPELINE_RUN_TS,\n",
    "            batch_id=int(batch_id),\n",
    "            started_ts_col=F.lit(started_ts).cast(\"timestamp\"),\n",
    "            finished_ts_col=F.lit(finished_ts).cast(\"timestamp\"),\n",
    "            rescued_col=\"_rescued_data\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# Start stream\n",
    "# -----------------------\n",
    "query = (\n",
    "    bronze_df.writeStream\n",
    "        .foreachBatch(process_batch)\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_LOCATION)\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# RUN END (once) - SUCCESS / EMPTY / FAILED\n",
    "# -----------------------\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "\n",
    "    # Determine EMPTY vs SUCCESS from streaming progress\n",
    "    total_in = 0\n",
    "    for p in query.recentProgress:\n",
    "        # progress payload is dict-like\n",
    "        try:\n",
    "            total_in += int(p.get(\"numInputRows\", 0))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    end_status = IngestStatus.EMPTY if total_in == 0 else IngestStatus.SUCCESS\n",
    "    end_msg = \"No new data files found\" if total_in == 0 else \"Stream completed successfully\"\n",
    "\n",
    "    write_log_best_effort(\n",
    "        run_event(\n",
    "            spark, LOG_CTX,\n",
    "            pipeline_run_ts_col=PIPELINE_RUN_TS,\n",
    "            batch_id=None,\n",
    "            status=end_status,\n",
    "            started_ts_col=PIPELINE_RUN_TS,\n",
    "            finished_ts_col=F.lit(datetime.now(timezone.utc)).cast(\"timestamp\"),\n",
    "            message=end_msg,\n",
    "        )\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # If the query fails, log once and re-raise\n",
    "    write_log_best_effort(\n",
    "        run_event(\n",
    "            spark, LOG_CTX,\n",
    "            pipeline_run_ts_col=PIPELINE_RUN_TS,\n",
    "            batch_id=None,\n",
    "            status=IngestStatus.FAILED,\n",
    "            started_ts_col=PIPELINE_RUN_TS,\n",
    "            finished_ts_col=F.lit(datetime.now(timezone.utc)).cast(\"timestamp\"),\n",
    "            message=str(e)[:4000],\n",
    "            error_class=e.__class__.__name__,\n",
    "        )\n",
    "    )\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5652313907494998,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
